# Training configuration

vocab_size: 32000
d_model: 512
num_heads: 8
num_layers: 6
max_seq_length: 128
dropout_rate: 0.1
learning_rate: 0.0001
batch_size: 64
num_epochs: 10
tokenizer_path: tokenizer
